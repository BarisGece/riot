= {app-name}
:favicon: images/favicon.svg
:app-name: RIOT Redis
:app: riot-redis
:resources: ../connectors/redis/src/test/resources
:toc: left
:numbered:
:toclevels: 2
:toc-title: Contents
:source-highlighter: coderay
:icons: font
:imagesdir: images
:linkattrs:
:sectanchors:
:docinfo: shared-head,private-head
ifdef::env-github[]
:caution-caption: :fire:
:important-caption: :heavy_exclamation_mark:
:note-caption: :information_source:
:tip-caption: :bulb:
:warning-caption: :warning:
endif::[]

== Overview

Most database migration tools available today are offline in nature. Migrating data from AWS ElastiCache to Redis Enterprise Cloud for example means backing up your Elasticache data to an AWS S3 bucket and importing it into Redis Enterprise Cloud using its UI.

This implies some downtime and might result in data loss.

Other available techniques include creating point-in-time snapshots of the source Redis server & applying the changes to the destination servers to keep both servers in sync.

It might sound like a good approach but can be challenging when you have to maintain dozens of scripts to implement the migration strategy.

*{app-name}* is a migration tool that allows for seamless live replication between two Redis databases.

include::getting_started.adoc[]

=== Replicating

The replication process takes data from a source Redis database and replicates it into a target Redis database. The general usage is:

[source,shell]
----
riot-redis -h <src host> -p <src port> replicate -h <dest host> -p <dest port> ...
----

Run `riot-redis replicate --help` for the complete usage help:

[source,shell]
----
riot-redis replicate --help
Usage: riot-redis replicate [OPTIONS]
Replicate a source Redis database to a target Redis database
  -H, --help                 Show this help message and exit.
      --[no-]progress        Show progress bars. True by default.
      --threads=<int>        Thread count (default: 1).
  -b, --batch=<size>         Number of items in each batch (default: 50).
      --max=<count>          Max number of items to read.
      --skip-limit=<int>     Max number of failed items to skip before the transfer fails (default: 0).
      --live                 Enable live replication.
      --flush-interval=<ms>  Max duration between flushes (default: 50).
      --idle-timeout=<ms>    Min duration of inactivity to consider transfer complete.
      --event-queue=<size>   Capacity of the keyspace notification event queue (default: 1000).
      --[no-]verify          Verify target against source dataset after replication. True by default.
      --ttl-tolerance=<sec>  Max TTL difference to use for dataset verification (default: 1).
Target Redis connection options
  -h, --hostname=<host>      Server hostname (default: localhost).
  -p, --port=<port>          Server port (default: 6379).
  -s, --socket=<socket>      Server socket (overrides hostname and port).
      --user=<username>      Used to send ACL style 'AUTH username pass'. Needs password.
  -a, --pass[=<password>]    Password to use when connecting to the server.
  -u, --uri[=<uri>...]       Server URI.
      --timeout=<sec>        Redis command timeout (default: 60).
  -n, --db=<db>              Database number (default: 0).
  -c, --cluster              Enable cluster mode.
      --tls                  Establish a secure TLS connection.
      --[no-]verify-peer     Verify peers when using TLS. True by default.
      --latency              Show latency metrics.
      --pool-max=<int>       Max pool connections (default: 8).
      --client=<clientName>  Client name used to connect to Redis.
Source Redis reader options
      --scan-count=<int>     SCAN COUNT option (default: 1000).
      --scan-match=<glob>    SCAN MATCH pattern (default: *).
      --reader-queue=<int>   Capacity of the reader queue (default: 1000).
      --reader-threads=<int> Number of reader threads (default: 1).
      --reader-batch=<int>   Number of reader values to process at once (default: 50).
----

.Example: live replication
[source,shell]
----
riot-redis -h source -p 6379 replicate --live -h target -p 6380
----

=== Verification

Once replication is complete (i.e. for snapshot replication when scan is complete, for live replication when no keyspace notification idle) *{app-name}* will perform a verification step to compare values and TTLs between source and target databases. The output looks like this:

----
OK:1000 V:0 >:0 <:0 T:0
----

* `OK`: # identical values
* `V`: # mismatched values
* `>`: # keys only present in source database
* `<`: # keys only present in target database
* `T`: # keys with TTL difference greater than tolerance

== Architecture

**{app-name}** implements client-side replication using a producer/consumer approach:

image::replication.png[]

<1> Key reader: initiates a `SCAN` and optionally calls `SUBSCRIBE` to listen for keyspace notifications (live replication).

<2> Value reader: takes the keys and calls `DUMP` and `TTL`.

<3> Key/Value writer: takes key/value/ttl tuples and calls `RESTORE` and `EXPIRE`.


=== Producer

The producer is connected to the source Redis (e.g. ElastiCache) database and iterates over keys to be replicated by performing a SCAN and reading the corresponding values with DUMP. This is the snapshot part of the replication process.

If live replication is enabled the producer also subscribes to keyspace notifications to generate a continuous stream of keys and corresponding values.

TIP: Make sure the source Redis database has keyspace notifications enabled using `notify-keyspace-events = KA` in `redis.conf` or via CONFIG SET.

WARNING: The live replication mechanism does not guarantee data consistency. Redis sends keyspace notifications over pub/sub which does not provide	guaranteed delivery. It is possible that {app-name} can miss some notifications in case of network failures for example.


Both snapshot and live producer processes make use of https://redis.io/topics/pipelining[pipelining] in order to read values in a batch fashion. Use the `--reader-batch` option to configure the number of values to be read in a single pipelined call. Each process can be multi-threaded using the `--reader-threads` option.

=== Consumer

The consumer is responsible for ingesting the keys and values generated by the producer and writing them to the target Redis database (e.g. Redis Enterprise Cloud). For each key/value pair it issues a RESTORE command.

The consumer process also makes use of https://redis.io/topics/pipelining[pipelining] in order to write values in a batch fashion. Use the `--batch` option to configure the number of values to be written in a single pipelined call.

Like the consumer the producer process can be multi-threaded, using the `--threads` option.